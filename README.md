# Agent-Testing-Framework

Agent-Testing-Framework is a CLI-based tool for evaluating the performance of Retrieval-Augmented Generation (RAG) agents and other AI models. It compares the responses generated by your AI agent to target responses using vector-based similarity, providing a quantitative measure of how closely your model matches the desired output.

## Features
- **CLI Interface**: Run evaluations directly from the command line.
- **Vector-Based Evaluation**: Uses sentence embeddings and cosine similarity to compare generated and target responses.
- **Customizable Thresholds**: Set similarity thresholds to determine pass/fail criteria.
- **Modular Design**: Easily extend or adapt for different models or evaluation strategies.

## Usage Instructions

### 1. Install Dependencies
Install the required Python packages:
```bash
pip install -r requirements.txt
```

### 2. Run the CLI
Use the CLI to evaluate your agent's responses. Example:
```bash
python cli.py --input generated_answers.json --target target_answers.json --threshold 0.8
```
- `--input`: Path to the file containing your agent's generated answers.
- `--target`: Path to the file with the target answers.
- `--threshold`: (Optional) Cosine similarity threshold for passing a test case (default: 0.8).

### 3. Review Results
The CLI will output a summary of pass/fail results and similarity scores for each test case.

## Backend Breakdown

- **embedder.py**: Contains the `EmbeddingEvaluator` class, which uses a sentence transformer model to embed text and compute cosine similarity between responses.
- **evaluator.py**: Implements the main evaluation logic, comparing generated and target answers using the embedding evaluator and applying the threshold to determine pass/fail status.
- **cli.py**: Provides the command-line interface for running evaluations, parsing arguments, and displaying results.
- **schemas.py**: Defines data schemas and structures used throughout the framework.

## Example Workflow
1. Prepare your generated and target answers in JSON format.
2. Run the CLI with the appropriate arguments.
3. Analyze the output to assess your agent's performance.

